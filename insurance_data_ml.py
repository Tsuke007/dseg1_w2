# -*- coding: utf-8 -*-
"""Insurance_data_ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ka9dPQHca41C3Rg-x9YV7woufWOE01Fa

Download dataset from: https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction
"""

!gdown --id 1E0eztrotQ9CPH5glLgONIwxV-M4WqXJ_

!unzip insurance_cross_sell.zip

"""#Pipeline
1. Collect Dataset
2. Clean Data
3. Extract Features
4. Split Training and test
5. Train
6. Evaluate
7. Create Pipeline

#1. Collect Dataset
"""

import pandas as pd
import numpy as np

insured_df = pd.read_csv('train.csv')

insured_df.head()

def collect_data():
  insured_df = pd.read_csv('train.csv')
  return insured_df

"""#2. Clean Data"""

insured_df.head()

insured_df.info()

insured_df['Response']

insured_df.isna().sum()

"""#3. Extract Features"""

insured_df.head()

def extract_feature(df, is_training=True):
  insured_new_df = df.copy()
  label_df = []

  if 'Response' in insured_new_df.columns:
    label_df = insured_new_df['Response']

     # drop unused columns
  if is_training:
    insured_new_df =insured_new_df.drop(['Response','id'],axis=1)
  col_names = ['Gender','Vehicle_Age','Vehicle_Damage']

    # One-Hot Encoding
  dummies_df = pd.get_dummies(insured_new_df[col_names])

   # Merge One-Hot Encoding
  insured_new_df = pd.concat([insured_new_df, dummies_df], axis=1)

  # Drop unused columns (One-Hot Encoding)
  insured_new_df = insured_new_df.drop(col_names, axis=1)


  for col in insured_new_df.columns:
    insured_new_df[col] = pd.to_numeric(insured_new_df[col],errors='coerce')

  return insured_new_df,label_df

insured_new_df,label_df = extract_feature(insured_df)

insured_new_df

"""#4. Train & Test split """

len(insured_new_df)

from sklearn.model_selection import train_test_split
train_insured, test_insured, train_label, test_label = \
          train_test_split(insured_new_df, label_df, test_size=0.2, random_state=42)

len(train_insured)

len(test_insured)

def split_train_test(insured_new_df, label):
  train_insured, test_insured, train_label, test_label = \
          train_test_split(insured_new_df, label, test_size=0.2, random_state=42)
  return train_insured, test_insured, train_label, test_label

train_insured, test_insured, train_label, test_label = \
        split_train_test(insured_new_df, label_df)

"""#5. Train (Classification)"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

def train_model(insured_new_df, label):
  model = GradientBoostingClassifier(random_state=2020)
  #model = SVC()
  model.fit(insured_new_df, label)
  return model

model = train_model(train_insured, train_label)

model

"""#6. Evaluation (Classification)"""

def eval_acc(prediction, actual):
  acc = sum(prediction == actual) / len(actual)
  return acc

pred = model.predict(test_insured)
eval_acc(pred, test_label)

train_insured

pred = model.predict(test_insured)
pred

test_label

"""#7. Create pipeline (Classification)"""

def run_pipeline():
  # collect data, clean data
  insured_df = collect_data()

  # Extract Feature
  train_df, test_df = extract_feature(insured_df) 
  
  # split training and test
  train_insured, test_insured, train_label, test_label = \
        split_train_test(insured_new_df, label_df)

  # Evaluation
  pred = model.predict(test_insured)
  acc = eval_acc(pred, test_label)

  return acc

print(run_pipeline())

model.save('Insurance.h5')